{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load biomass data\n",
    "def load_biomass_data(year, shape, resolution=250):\n",
    "    path_bio = f\"../data/raw/biomass/{resolution}m/\" + f\"mapbiomas-brazil-collection-70-acre-{year}.tif\"\n",
    "    with rasterio.open(path_bio) as src:\n",
    "        if shape is not None:\n",
    "            bio_data, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "        else:\n",
    "            bio_data = src.read(1)\n",
    "        bio_data = np.squeeze(bio_data)\n",
    "        out_meta = src.meta\n",
    "    return bio_data\n",
    "\n",
    "# function to transform labels to 1=forest, 2=non_forest, 0=unknown\n",
    "def transform_to_labels(bio_data):\n",
    "    class_dict = {1:0, 3:0, 4:0, 5:0,49:0, # forest\n",
    "                10:1,11:1,12:1,32:1,29:1,13:1, 13:1, 50:1, # natural\n",
    "                14:1,15:1,18:1,19:1,39:1,20:1,40:1,61:1,41:1,36:1,46:1,47:1,48:1,9:1,21:1, # farming\n",
    "                22:1,23:1,24:1,30:1,25:1, # urban\n",
    "                26:1,33:1,31:1, # water\n",
    "                27:255,0:255} # unobserved\n",
    "    bio_data_new = np.zeros_like(bio_data)\n",
    "    for key, value in class_dict.items():\n",
    "        bio_data_new[bio_data == key] = value\n",
    "    return np.array(bio_data_new, dtype=np.uint8)\n",
    "\n",
    "def process_data(start_year, nr_years_train, horizon, resolution):\n",
    "    # parameters to load date\n",
    "    start_train = start_year # start of training data\n",
    "    start_target = start_year + nr_years_train  # model has to predict this and following horizon\n",
    "    start_test = start_target + horizon # model won't see anything after this year\n",
    "\n",
    "    # load all the different years\n",
    "    years = np.arange(start_train, start_test+horizon)\n",
    "\n",
    "    path_bio_processed = f\"../data/processed/biomass/{resolution}m/\"\n",
    "    bio_data_dict = {}\n",
    "    for year in years:\n",
    "        if not os.path.exists(path_bio_processed + f\"biomass_{year}.pt\"):\n",
    "            bio_data = load_biomass_data(year, None, resolution=resolution)\n",
    "            bio_data = transform_to_labels(bio_data)\n",
    "            torch.save(torch.from_numpy(bio_data), path_bio_processed + f\"biomass_{year}.pt\")\n",
    "        else:\n",
    "            bio_data = torch.load(path_bio_processed + f\"biomass_{year}.pt\").numpy()\n",
    "        bio_data_dict[year] = bio_data\n",
    "\n",
    "    # calculate deforestation\n",
    "    deforestation = np.zeros_like(bio_data_dict[start_target - 1], dtype=bool)\n",
    "    for year in np.arange(start_target, start_target+horizon):\n",
    "        deforested = bio_data_dict[start_target - 1] - bio_data_dict[year]\n",
    "        deforestation = deforestation | (deforested == 255)\n",
    "\n",
    "    # get coordinates\n",
    "    x = np.arange(deforestation.shape[1])\n",
    "    y = np.arange(deforestation.shape[0])\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "    data = [xv.flatten(), yv.flatten(), deforestation.flatten()]\n",
    "    for year in np.arange(start_target, start_test+horizon):\n",
    "        data.append(bio_data_dict[year].flatten())\n",
    "    \n",
    "    dtype = np.int16 if np.max(deforestation.shape) <= 32767 else np.int32\n",
    "    data = np.array(data, dtype=dtype).T\n",
    "    data = data[bio_data_dict[start_target - 1].flatten() == 0] # ensure forest cover in last input year\n",
    "    data = data[np.max(data[:,3:],axis=1) <= 1] # filter out future non-observed points\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2006\n",
    "nr_years_train = 10\n",
    "horizon = 3\n",
    "\n",
    "# parameters to load date\n",
    "start_train = start_year # start of training data\n",
    "start_target = start_year + nr_years_train  # model has to predict this and following horizon\n",
    "start_test = start_target + horizon # model won't see anything after this year\n",
    "\n",
    "# load all the different years\n",
    "years = np.arange(start_target - 1, start_test+horizon)\n",
    "\n",
    "bio_data_dict = {}\n",
    "for year in years:\n",
    "    bio_data = load_biomass_data(year, None, resolution=30)\n",
    "    bio_data = transform_to_labels(bio_data)\n",
    "    bio_data_dict[year] = bio_data\n",
    "\n",
    "# calculate deforestation\n",
    "deforestation = np.zeros_like(bio_data_dict[start_target - 1], dtype=bool)\n",
    "for year in np.arange(start_target, start_target+horizon):\n",
    "    deforested = bio_data_dict[start_target - 1] - bio_data_dict[year]\n",
    "    deforestation = deforestation | (deforested == 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates\n",
    "x = np.arange(deforestation.shape[1])\n",
    "y = np.arange(deforestation.shape[0])\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "data = [xv.flatten(), yv.flatten(), deforestation.flatten()]\n",
    "for year in np.arange(start_target, start_test+horizon):\n",
    "    data.append(bio_data_dict[year].flatten())\n",
    "\n",
    "dtype = np.int16 if np.max(deforestation.shape) <= 32767 else np.int32\n",
    "data = np.array(data, dtype=dtype).T\n",
    "data = data[bio_data_dict[start_target - 1].flatten() == 0] # ensure forest cover in last input year\n",
    "data = data[np.max(data[:,3:],axis=1) <= 1] # filter out future non-observed points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class DeforestationDataset(Dataset):\n",
    "    def __init__(self, dataset, start_year=2006, nr_years_train=10, horizon=3, resolution=30, patch_size=35):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: \"train\", \"val\", \"test\"\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.start_year = start_year\n",
    "        self.nr_years_train = nr_years_train\n",
    "        self.horizon = horizon\n",
    "        self.resolution = resolution\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # dataformat: [x_idx, y_idx, deforested, 2016, 2017, ..., 2021]\n",
    "        self.data = torch.load(f'../data/processed/{dataset}_data.pt')\n",
    "\n",
    "        self.bio_data = {}\n",
    "        path_bio_processed = f\"../data/processed/biomass/{resolution}m/\"\n",
    "        for year in np.arange(self.start_year, self.start_year + self.nr_years_train + 2 * horizon):\n",
    "            self.bio_data[year] = torch.load(path_bio_processed + f\"biomass_{year}.pt\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset == \"test\":\n",
    "            labels = self.data[idx, 3+self.horizon:]\n",
    "            years = np.arange(self.start_year + self.horizon, self.start_year + self.horizon + self.nr_years_train)\n",
    "        else:\n",
    "            labels = self.data[idx, 3:3+self.horizon]\n",
    "            years = np.arange(self.start_year, self.start_year + self.nr_years_train)\n",
    "\n",
    "        x_idx = self.data[idx, 0]\n",
    "        y_idx = self.data[idx, 1]\n",
    "        r = int(self.patch_size/2)\n",
    "\n",
    "        features = []\n",
    "        for year in years:\n",
    "            window = self.bio_data[year][y_idx-r:y_idx+r, x_idx-r:x_idx+r]\n",
    "            features.append(window)\n",
    "        features = torch.stack(features)\n",
    "\n",
    "        return features.float(), labels.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeforestationDataset(\"train\", 2006, 10, 3, 30, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x8f4141e70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(train_dataset.__getitem__(100)[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders():\n",
    "    # load datasets\n",
    "    train_dataset = DeforestationDataset(\"train\")\n",
    "    val_dataset = DeforestationDataset(\"val\")\n",
    "    test_dataset = DeforestationDataset(\"test\")\n",
    "\n",
    "    # create the train, val and test dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=1)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=1)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    input_dim=10\n",
    "    hidden_dim=[128, 64, 64, 32]\n",
    "    kernel_size=[(5, 5), (5, 5), (3, 3), (3, 3)]\n",
    "    stride=[(2, 2), (1, 1), (1, 1), (1, 1)]\n",
    "    padding=[0, 0, 0, 0]\n",
    "    dropout=0.2\n",
    "    output_dim=3\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # convolutional blocks\n",
    "    for i in range(len(hidden_dim)):\n",
    "        layer_input_dim = input_dim if i == 0 else hidden_dim[i-1]\n",
    "        layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "            layer_input_dim,\n",
    "            hidden_dim[i],\n",
    "            kernel_size[i],\n",
    "            stride=stride[i],\n",
    "            padding=padding[i],\n",
    "        ))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.BatchNorm2d(hidden_dim[i]))\n",
    "\n",
    "    layers.append(torch.nn.MaxPool2d(2, 2, 0))\n",
    "    layers.append(torch.nn.Dropout(dropout))\n",
    "    layers.append(torch.nn.Flatten())\n",
    "\n",
    "    lin_in = 288 # maybe find way to calculate this\n",
    "    layers.append(torch.nn.Linear(lin_in, 100))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.BatchNorm1d(100))\n",
    "    layers.append(torch.nn.Dropout(dropout))\n",
    "    layers.append(torch.nn.Linear(100, output_dim))\n",
    "\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.8.dylib\n",
      "  Referenced from: <BE0CCD9A-269A-30E2-A23C-DA45E89EBB1F> /opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torchvision/../../../libjpeg.8.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torchvision/../../../libjpeg.8.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/lib-dynload/../../libjpeg.8.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/bin/../lib/libjpeg.8.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS@rpath/libjpeg.8.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torchvision/../../../libjpeg.8.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torchvision/../../../libjpeg.8.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/lib-dynload/../../libjpeg.8.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/bin/../lib/libjpeg.8.dylib' (no such file), '/usr/local/lib/libjpeg.8.dylib' (no such file), '/usr/lib/libjpeg.8.dylib' (no such file, not in dyld cache)\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "class ForestModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = compile_model()\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss() # weights disregarded\n",
    "\n",
    "        self.auroc = torchmetrics.AUROC(task=\"binary\")\n",
    "        self.prec = torchmetrics.Precision(task=\"binary\")\n",
    "        self.recall = torchmetrics.Recall(task=\"binary\")\n",
    "        self.f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "\n",
    "    def forward(self, features):\n",
    "        logits = self.model(features)\n",
    "        return logits\n",
    "    \n",
    "    def shared_step(self, batch, stage):\n",
    "\n",
    "        features = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        logits = self.forward(features)\n",
    "\n",
    "        logits_loss = logits.unsqueeze(1)\n",
    "        target_loss = target.unsqueeze(1)\n",
    "        loss = self.loss_fn(logits_loss, target_loss)\n",
    "        probs = logits.sigmoid()\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        metrics_batch = {}\n",
    "        for i in range(preds.shape[1]):\n",
    "            metrics_batch[f\"auc_{i}\"] = self.auroc(probs[:,i], target[:,i])\n",
    "            metrics_batch[f\"precision_{i}\"] = self.prec(preds[:,i], target[:,i])\n",
    "            metrics_batch[f\"recall_{i}\"] = self.recall(preds[:,i], target[:,i])\n",
    "            metrics_batch[f\"f1_{i}\"] = self.f1(preds[:,i], target[:,i])\n",
    "        \n",
    "        metrics_batch[\"loss\"] = loss\n",
    "\n",
    "        return metrics_batch\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "\n",
    "        metrics_epoch = {}\n",
    "        for key, value in outputs[0].items():\n",
    "            metrics_epoch[f\"{stage}_{key}\"] = torch.stack([x[key] for x in outputs]).mean()\n",
    "\n",
    "        self.log_dict(metrics_epoch, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")            \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"test\")  \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ForestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | Sequential       | 322 K \n",
      "1 | loss_fn | CrossEntropyLoss | 0     \n",
      "2 | auroc   | BinaryAUROC      | 0     \n",
      "3 | prec    | BinaryPrecision  | 0     \n",
      "4 | recall  | BinaryRecall     | 0     \n",
      "5 | f1      | BinaryF1Score    | 0     \n",
      "---------------------------------------------\n",
      "322 K     Trainable params\n",
      "0         Non-trainable params\n",
      "322 K     Total params\n",
      "1.289     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699036dfc8484d1e850b2b00b459f15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "      File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'DeforestationDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 30385) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 30385) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,\n\u001b[1;32m      5\u001b[0m     log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      9\u001b[0m     model, \n\u001b[1;32m     10\u001b[0m     train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader, \n\u001b[1;32m     11\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mval_loader\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1203\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1206\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1276\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1280\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:121\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    120\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n\u001b[0;32m--> 121\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    266\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    281\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/semester_project/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 30385) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator='mps', \n",
    "    devices=1,\n",
    "    max_epochs=30,\n",
    "    log_every_n_steps=5\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=val_loader\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ForestModel.load_from_checkpoint(\"../lightning_logs/version_2/checkpoints/epoch=9-step=320.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator='mps', \n",
    "    devices=1,\n",
    "    max_epochs=30,\n",
    "    log_every_n_steps=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semester_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
